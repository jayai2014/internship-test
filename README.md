# Silverpond Internship Test

## Hello!

We'd like you to prepare some code in a [Jupyter
Notebook](http://jupyter.org/) that explains a concept in machine
learning/AI/deep learning that is of interest to you. You may use
[TensorFlow](https://www.tensorflow.org/), [PyTorch](http://pytorch.org/), or
the framework of your choice; as long as it is in notebook form.

As a team we're particularly interested in, and will be looking for:

- Good explanations,
- Modular and Idiomatic code,
- Clear goals,
- Tying-back final output to original goals,
- Different viewpoints,
- Fundamental understanding,
- Awareness of knowledge boundaries (_"What do I know about how much I know!"_).

As a reference, your notebook need not be any longer than the one that we use
for the prerequisite for the deep learning workshop:

- [Python Prerequisites for the Deep Learning
Workshop](https://github.com/silverpond/dl-workshop-pre-req/blob/master/Pre-Requesites.ipynb)

We would like this notebook to be hosted on your own GitHub (or other public
git source control system; we want to know you can use Git!). Don't worry if
it's the only thing on your account!

Please try not to spend more than 6 hours; so either try and explain well a
concept you're already familiar with, or peform a brief investigation into the
open areas of a interesting technique!

### Potential example projects:

Here are some ideas that should indicate the general direction we're
interested in. Please feel free to come up with your own!

1. _Playing with convolutions_: A notebook that looks at convolutions, purely
   independently of deep learning, and tests a few different varieties on
   images.

2. _Algebra on auto-encoders_: A notebook that loads up an existing auto-encoder
   model and performs some algebra within it, to demonstrate what it's
   capabilities are.

3. _t-SNE_: A notebook that takes a model that produces some latent vector
    `z`, and performs t-SNE on it, then explains what the t-SNE is
    doing.

4. _Bayesian probability_: A notebook that demonstrates how bayesian probability
   works, and some examples of prior-updates, and computations of the
   probabilities in a few different scenarios.

5. _Bias in ML_: A notebook that explains how bias can be found in ML models,
   and highlights a few ways that people are attempting to solve this problem.

6. _Precision scores_: An overview of the different measures that people use
   to assess the capability of different ML models.

7. _An Exploration of the TensorFlow Tensor_: An review of what Tensors are,
   in the context of TensorFlow, and the various operations that can be
   performed on them, and what their affect is.
